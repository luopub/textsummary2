{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"work.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"WKuYnu6ij7V6","colab_type":"code","colab":{}},"source":["# !cd /content\n","# !pwd\n","# !ls /content"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"sWYxoKtPiwJj","colab_type":"code","colab":{}},"source":["# !pip install jieba\n","# !pip install gensim"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OgumTPAkiwJp","colab_type":"code","outputId":"76a09122-72f9-431d-f961-cdc2fd2f7228","executionInfo":{"status":"ok","timestamp":1576859940147,"user_tz":-480,"elapsed":39120,"user":{"displayName":"Luo Yongliang","photoUrl":"","userId":"08228183385143629193"}},"colab":{"base_uri":"https://localhost:8080/","height":145}},"source":["# import pdb\n","\n","from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive\n","\n","data_root = '/gdrive/My Drive/data_tf'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n","/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V3cKLpf1iwJt","colab_type":"code","outputId":"d7f16789-ff6f-4f4a-e94e-65526cb37c21","executionInfo":{"status":"ok","timestamp":1576859945712,"user_tz":-480,"elapsed":44671,"user":{"displayName":"Luo Yongliang","photoUrl":"","userId":"08228183385143629193"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Above is for notebook only\n","# Below is to be moved to py files\n","! ls '/gdrive/My Drive/python-code/文本摘要-01'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["data  utils  work.ipynb  work-tf-unigru-attn.ipynb\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dUq4JDg2iwJw","colab_type":"text"},"source":["# 上面是测试用\n","# ============================================================\n","# 下面要转移到文件里面"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"VDXlPSnDiwJx","colab_type":"code","outputId":"8381f4ff-fc9b-4979-c43b-9f285500d1dd","executionInfo":{"status":"error","timestamp":1576859954903,"user_tz":-480,"elapsed":6465,"user":{"displayName":"Luo Yongliang","photoUrl":"","userId":"08228183385143629193"}},"colab":{"base_uri":"https://localhost:8080/","height":515}},"source":["# import modules\n","import os\n","import sys\n","import pathlib\n","import pandas as pd\n","import numpy as np\n","import pdb\n","import time\n","\n","project_root = pathlib.Path(r'/gdrive/My Drive/python-code/文本摘要-01')\n","sys.path.append(str(project_root))\n","\n","from utils.config import *\n","from utils.multi_proc_utils import *\n","from utils.data_loader import *\n","from utils.file_utils import *\n","from utils.data_preprocessor import *"],"execution_count":3,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-5ddf19bd740d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_proc_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_preprocessor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/gdrive/My Drive/python-code/文本摘要-01/utils/data_loader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_proc_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparallelize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_seg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_seg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerger_seg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x_seg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x_seg_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_x_pad_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_pad_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x_pad_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_train_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvocab_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse_vocab_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'embedding_dim'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"6v3rNBzliwJ0","colab_type":"text"},"source":["# Create data preprocessor"]},{"cell_type":"code","metadata":{"id":"nhKL-XjNiwJ2","colab_type":"code","outputId":"7d6927f1-c9fd-4bb1-d24f-c30d6b2a5bca","executionInfo":{"status":"error","timestamp":1576859954904,"user_tz":-480,"elapsed":3324,"user":{"displayName":"Luo Yongliang","photoUrl":"","userId":"08228183385143629193"}},"colab":{"base_uri":"https://localhost:8080/","height":245}},"source":["wv_train_epochs = 1\n","wv_update_epochs = 1\n","\n","working_ds = AutoCarDataSet()\n","working_ds.prepare_data(force_build=False)\n","working_ds.get_wv_model(force_build=False)"],"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-4d79bf9714be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwv_update_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mworking_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoCarDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mworking_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_build\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mworking_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_wv_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_build\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'AutoCarDataSet' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"UU35fUpViwJ7","colab_type":"text"},"source":["# 以下是tensorflow实现带attention 的seq2seq============="]},{"cell_type":"code","metadata":{"id":"BS4CY9kviwJ8","colab_type":"code","outputId":"9c26bb21-e298-4934-f352-fde1a4ee0e09","executionInfo":{"status":"ok","timestamp":1576832195526,"user_tz":-480,"elapsed":100010,"user":{"displayName":"Luo Yongliang","photoUrl":"","userId":"08228183385143629193"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","# define constants for codec\n","BUFFER_SIZE = working_ds.train_ids_x.shape[0]\n","BATCH_SIZE = 16\n","steps_per_epoch = working_ds.train_ids_x.shape[0]//BATCH_SIZE\n","embedding_dim = embedding_dim\n","codec_units = 256\n","vocab_inp_size = len(working_ds.vocab)\n","vocab_tar_size = len(working_ds.vocab)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eg_zf-WdiwJ_","colab_type":"code","colab":{}},"source":["class UnidirGruEncoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, word_matrix, enc_units, batch_sz):\n","    super(UnidirGruEncoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.enc_units = enc_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights = [word_matrix])\n","    self.gru = tf.keras.layers.GRU(self.enc_units,\n","            return_sequences=True,\n","            return_state=True,\n","            recurrent_initializer='glorot_uniform')\n","    \n","  def initialize_hidden_state(self):\n","    return tf.zeros((self.batch_sz, self.enc_units))\n","\n","  def call(self, x, hidden):\n","    # debug_print(\"input x.shape 1: \", x.shape)\n","    # debug_print(\"input hidden: \", hidden.shape)\n","    \n","    x = self.embedding(x)\n","    # debug_print(\"input x.shape 2: \", x.shape)\n","    \n","    output, state = self.gru(x, initial_state = hidden)\n","    # debug_print(\"output.shape: \", output.shape)\n","    # debug_print(\"state.shape: \", state.shape)\n","    \n","    return output, state"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nhN8BpqiwKB","colab_type":"code","colab":{}},"source":["class BahdanauAttention(tf.keras.Model):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, query, values):\n","    # debug_print(\"query.shape; \", query.shape)\n","    # debug_print(\"values.shape; \", values.shape)\n","\n","    # we are doing this to perform addition to calculate the score\n","    hidden_with_time_axis = tf.expand_dims(query, 1)\n","    # debug_print(\"hidden_with_time_axis.shape; \", hidden_with_time_axis.shape)\n","\n","    # we get 1 at the last axis because we are applying score to self.V\n","    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","    w1 = self.W1(values)\n","    # debug_print(\"w1.shape; \", w1.shape)\n","\n","    w2 = self.W2(hidden_with_time_axis)\n","    # debug_print(\"w2.shape; \", w2.shape)\n","\n","    sum_w12 = w1 + w2\n","    # debug_print(\"sum_w12.shape; \", sum_w12.shape)\n","\n","    aw12 = tf.nn.tanh(sum_w12)\n","    # debug_print(\"aw12.shape; \", aw12.shape)\n","\n","    score = self.V(aw12)\n","    # debug_print(\"score.shape; \", score.shape)\n","\n","    # attention_weights shape == (batch_size, max_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","    # debug_print(\"attention_weights.shape; \", attention_weights.shape)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * values\n","    # debug_print(\"context_vector.shape; \", context_vector.shape)\n","\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","    # debug_print(\"context_vector.shape; \", context_vector.shape)\n","\n","    return context_vector, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4q6kG2wEiwKD","colab_type":"code","colab":{}},"source":["class UniGruDecoderWithAttention(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, word_matrix, dec_units, batch_sz):\n","    super(UniGruDecoderWithAttention, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.dec_units = dec_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights = [word_matrix])\n","    self.gru = tf.keras.layers.GRU(self.dec_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","    # used for attention\n","    self.attention = BahdanauAttention(self.dec_units)\n","\n","  def call(self, x, hidden, enc_output):\n","    # enc_output shape == (batch_size, max_length, hidden_size)\n","    context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    context_vector = tf.expand_dims(context_vector, 1)\n","    # debug_print(\"context_vector expanded shape; \", context_vector.shape)\n","\n","    x = tf.concat([context_vector, x], axis=-1)\n","    # debug_print(\"x concated context_vector shape; \", x.shape)\n","\n","    # passing the concatenated vector to the GRU\n","    output, state = self.gru(x)\n","    # debug_print(\"gru output.shape; \", output.shape)\n","    # debug_print(\"gru state.shape; \", state.shape)\n","\n","    # output shape == (batch_size * 1, hidden_size)\n","    output = tf.reshape(output, (-1, output.shape[2]))\n","    # debug_print(\"fc input shape; \", output.shape)\n","\n","    # output shape == (batch_size, vocab)\n","    x = self.fc(output)\n","    # debug_print(\"fc output shape; \", x.shape)\n","\n","    return x, state, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ReJVTU5KiwKG","colab_type":"code","colab":{}},"source":["optimizer = tf.keras.optimizers.Adam(1e-2)\n","\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n7tyU_x0iwKL","colab_type":"code","colab":{}},"source":["def train_one_batch(encoder, decoder, inp, targ, enc_hidden):\n","  loss = 0\n","\n","  # debug_print(\"train_step inp.shape: \", inp.shape)\n","  # debug_print(\"train_step targ.shape: \", targ.shape)\n","  with tf.GradientTape() as tape:\n","    enc_output, enc_hidden = encoder(inp, enc_hidden)\n","\n","    dec_hidden = enc_hidden\n","\n","    dec_input = tf.expand_dims([working_ds.vocab['<START>']] * BATCH_SIZE, 1)\n","    # debug_print(\"train_step dec_input.shape: \", dec_input.shape)\n","\n","    # Teacher forcing - feeding the target as the next input\n","    for t in range(1, targ.shape[1]):\n","      # passing enc_output to the decoder\n","      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","\n","      loss += loss_function(targ[:, t], predictions)\n","\n","      # using teacher forcing\n","      dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","  # debug_print(\"train_step predictions.shape: \", predictions.shape)\n","  # debug_print(\"train_step loss.shape: \", loss.shape)\n","\n","  batch_loss = (loss / int(targ.shape[1]))\n","\n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","  # debug_print(\"train_step variables.shape: \", variables.shape)\n","\n","  gradients = tape.gradient(loss, variables)\n","  # debug_print(\"train_step gradients.shape: \", gradients.shape)\n","\n","  zipped_variables = zip(gradients, variables)\n","  # debug_print(\"zipped_variables.shape: \", zipped_variables.shape)\n","  optimizer.apply_gradients(zipped_variables)\n","\n","  return batch_loss\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Icojm4qoiwKR","colab_type":"code","colab":{}},"source":["#@tf.function\n","def train_one_epoch(encoder, decoder, dataset):\n","    enc_hidden = encoder.initialize_hidden_state()\n","    total_loss = 0.0\n","\n","    starttime = time.time()\n","    print('Epoch start time {}'.format(starttime))\n","    loss_history = []\n","    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","        batch_loss = train_one_batch(encoder, decoder, inp, targ, enc_hidden)\n","        # pdb.set_trace()\n","        total_loss += batch_loss\n","\n","        loss_history.append(batch_loss)\n","\n","        # if batch == 5:\n","        #   break\n","\n","        if (batch+1) % 10 == 0:\n","            # print('Batch {} time {:.2f}'.format(batch, time.time() - starttime))\n","            print('Batch {} Loss {:.4f} time {:.2f}'.format(batch+1, batch_loss.numpy(), time.time() - starttime)) #Not valid if @tf.function\n","    print('Loss at epoch end {:.4f}'.format(total_loss / steps_per_epoch)) #Not valid if @tf.function\n","\n","    return total_loss / steps_per_epoch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VcVObWQqiwKU","colab_type":"code","colab":{}},"source":["def def_checkpoint(encoder, decoder, optimizer):\n","    # checkpoint_dir = checkpoint_dir_tf\n","    # checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_unigru_attn\")\n","    checkpoint = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n","    return checkpoint\n","\n","def create_codec():\n","    encoder = UnidirGruEncoder(vocab_inp_size, embedding_dim, working_ds.embedding_matrix, codec_units, BATCH_SIZE)\n","    decoder = UniGruDecoderWithAttention(vocab_tar_size, embedding_dim, working_ds.embedding_matrix, codec_units, BATCH_SIZE)\n","    return encoder, decoder\n","\n","def get_dataset():\n","    dataset = tf.data.Dataset.from_tensor_slices((working_ds.train_ids_x, working_ds.train_ids_y)).shuffle(BUFFER_SIZE)\n","    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","    return dataset\n","\n","def train_all(encoder, decoder, dataset, epochs, save_chkpnts):\n","\n","    if save_chkpnts:\n","        checkpoint = def_checkpoint(encoder, decoder, optimizer)\n","    else:\n","        checkpoint = None\n","\n","    loss_history = []\n","    for e in range(epochs):\n","        print('Epoch {} ============================>'.format(e))\n","        epoch_loss = train_one_epoch(encoder, decoder, dataset)\n","        loss_history.append(epoch_loss)\n","        \n","        if checkpoint  and (e + 1) % 1 == 0:\n","            # saving (checkpoint) the model every 2 epochs\n","            checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","    return loss_history"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1AbANfJSIwOJ","colab_type":"code","colab":{}},"source":["def document_as_tensor(doc):\n","    doc = sentence_proc(doc)\n","    doc = pad_proc(doc, working_ds.X_max_len, working_ds.vocab)\n","    doc = transform_data(doc, working_ds.vocab)\n","    doc = tf.convert_to_tensor(doc)\n","    return doc\n","\n","def evaluate(encoder, decoder, doc):\n","    attention_plot = np.zeros((working_ds.X_max_len, working_ds.train_y_max_len))\n","\n","    inputs = document_as_tensor(doc)\n","\n","    result = ''\n","\n","    hidden = [tf.zeros((1, codec_units))]\n","    enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([working_ds.vocab['<START>']], 0)\n","\n","    for t in range(working_ds.train_y_max_len):\n","        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n","\n","        # storing the attention weights to plot later on\n","        attention_weights = tf.reshape(attention_weights, (-1, ))\n","        attention_plot[t] = attention_weights.numpy()\n","\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","        result += working_ds.reverse_vocab[predicted_id] + ' '\n","\n","        if working_ds.reverse_vocab[predicted_id] == '<STOP>':\n","            return result, sentence, attention_plot\n","\n","        # the predicted ID is fed back into the model\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    return result, inputs, attention_plot"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ZUw9oNHbIGz","colab_type":"code","colab":{}},"source":["def train_for_restore(encoder, decoder, dataset):\n","  enc_hidden = encoder.initialize_hidden_state()\n","\n","  inp, targ = next(iter(dataset))\n","  batch_loss = train_one_batch(encoder, decoder, inp, targ, enc_hidden)\n","\n","  loss = 0\n","  with tf.GradientTape() as tape:\n","    enc_output, enc_hidden = encoder(inp, enc_hidden)\n","\n","    dec_hidden = enc_hidden\n","\n","    dec_input = tf.expand_dims([working_ds.vocab['<START>']] * BATCH_SIZE, 1)\n","    # debug_print(\"train_step dec_input.shape: \", dec_input.shape)\n","\n","    # Teacher forcing - feeding the target as the next input\n","    for t in range(1, targ.shape[1]):\n","      # passing enc_output to the decoder\n","      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","\n","      loss += loss_function(targ[:, t], predictions)\n","\n","      # using teacher forcing\n","      dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","  batch_loss = (loss / int(targ.shape[1]))\n","\n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  gradients = tape.gradient(loss, variables)\n","\n","  zipped_variables = zip(gradients, variables)\n","  optimizer.apply_gradients(zipped_variables)\n","\n","  return batch_loss\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VvcR3LjqiwKW","colab_type":"text"},"source":["# 这里是上下分隔符，下面都是测试代码======================="]},{"cell_type":"code","metadata":{"id":"4esqlYK_iwKX","colab_type":"code","outputId":"877c8674-9b79-4f36-9507-aaa0296b0924","executionInfo":{"status":"ok","timestamp":1576836703161,"user_tz":-480,"elapsed":4471500,"user":{"displayName":"Luo Yongliang","photoUrl":"","userId":"08228183385143629193"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["encoder, decoder = create_codec()\n","\n","dataset = get_dataset()\n","loss_history = train_all(encoder, decoder, dataset, 1, True)\n","encoder.summary()\n","decoder.summary()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0 ============================>\n","Epoch start time 1576832240.597346\n","Batch 10 Loss 3.8703 time 15.32\n","Batch 20 Loss 3.0178 time 24.06\n","Batch 30 Loss 2.3273 time 32.69\n","Batch 40 Loss 2.1977 time 41.20\n","Batch 50 Loss 2.5271 time 49.98\n","Batch 60 Loss 2.6982 time 58.90\n","Batch 70 Loss 2.3343 time 67.66\n","Batch 80 Loss 2.6145 time 76.07\n","Batch 90 Loss 2.4875 time 84.76\n","Batch 100 Loss 2.6055 time 93.66\n","Batch 110 Loss 2.2349 time 101.85\n","Batch 120 Loss 2.4149 time 110.69\n","Batch 130 Loss 2.5225 time 119.37\n","Batch 140 Loss 1.7834 time 128.22\n","Batch 150 Loss 2.3185 time 136.49\n","Batch 160 Loss 2.4661 time 145.19\n","Batch 170 Loss 2.6094 time 153.91\n","Batch 180 Loss 3.0307 time 162.53\n","Batch 190 Loss 2.6528 time 171.19\n","Batch 200 Loss 2.2771 time 179.96\n","Batch 210 Loss 2.8102 time 188.67\n","Batch 220 Loss 3.5790 time 196.89\n","Batch 230 Loss 2.2124 time 205.89\n","Batch 240 Loss 2.4412 time 214.65\n","Batch 250 Loss 2.1005 time 223.03\n","Batch 260 Loss 2.2075 time 231.71\n","Batch 270 Loss 2.3308 time 240.60\n","Batch 280 Loss 2.0358 time 249.36\n","Batch 290 Loss 2.7131 time 257.76\n","Batch 300 Loss 2.2769 time 266.59\n","Batch 310 Loss 1.6209 time 275.19\n","Batch 320 Loss 1.9113 time 283.62\n","Batch 330 Loss 2.5742 time 292.46\n","Batch 340 Loss 1.8930 time 301.35\n","Batch 350 Loss 3.0287 time 310.47\n","Batch 360 Loss 2.3295 time 318.88\n","Batch 370 Loss 2.2338 time 327.57\n","Batch 380 Loss 2.1041 time 336.18\n","Batch 390 Loss 1.9650 time 344.56\n","Batch 400 Loss 2.3901 time 353.26\n","Batch 410 Loss 2.7057 time 361.99\n","Batch 420 Loss 2.5818 time 370.88\n","Batch 430 Loss 2.0920 time 379.17\n","Batch 440 Loss 2.1934 time 387.96\n","Batch 450 Loss 1.5156 time 396.87\n","Batch 460 Loss 2.3540 time 405.14\n","Batch 470 Loss 2.8185 time 413.87\n","Batch 480 Loss 2.2278 time 422.70\n","Batch 490 Loss 1.8048 time 431.59\n","Batch 500 Loss 2.5318 time 439.92\n","Batch 510 Loss 2.2577 time 448.78\n","Batch 520 Loss 2.5705 time 457.63\n","Batch 530 Loss 2.6457 time 466.08\n","Batch 540 Loss 2.2562 time 474.77\n","Batch 550 Loss 2.5138 time 483.50\n","Batch 560 Loss 2.1562 time 492.24\n","Batch 570 Loss 2.4917 time 500.69\n","Batch 580 Loss 1.9287 time 509.31\n","Batch 590 Loss 2.4898 time 518.13\n","Batch 600 Loss 2.9564 time 526.45\n","Batch 610 Loss 1.5930 time 535.20\n","Batch 620 Loss 2.3927 time 544.05\n","Batch 630 Loss 2.0953 time 552.67\n","Batch 640 Loss 1.4297 time 561.04\n","Batch 650 Loss 1.6714 time 569.98\n","Batch 660 Loss 1.8567 time 578.75\n","Batch 670 Loss 1.9781 time 586.96\n","Batch 680 Loss 2.2045 time 595.75\n","Batch 690 Loss 1.8653 time 604.49\n","Batch 700 Loss 1.9873 time 613.14\n","Batch 710 Loss 2.1816 time 621.99\n","Batch 720 Loss 1.6636 time 630.96\n","Batch 730 Loss 1.9595 time 639.73\n","Batch 740 Loss 1.8982 time 647.99\n","Batch 750 Loss 2.5334 time 656.65\n","Batch 760 Loss 1.6685 time 665.39\n","Batch 770 Loss 1.8444 time 674.17\n","Batch 780 Loss 1.9865 time 682.31\n","Batch 790 Loss 2.3522 time 690.88\n","Batch 800 Loss 2.5889 time 699.36\n","Batch 810 Loss 2.0851 time 707.70\n","Batch 820 Loss 1.9345 time 716.46\n","Batch 830 Loss 2.7494 time 725.22\n","Batch 840 Loss 2.4539 time 733.85\n","Batch 850 Loss 1.9442 time 742.02\n","Batch 860 Loss 2.1768 time 750.85\n","Batch 870 Loss 2.1399 time 759.44\n","Batch 880 Loss 2.3095 time 767.63\n","Batch 890 Loss 2.3818 time 776.34\n","Batch 900 Loss 2.7427 time 785.13\n","Batch 910 Loss 2.1454 time 793.76\n","Batch 920 Loss 1.9717 time 801.96\n","Batch 930 Loss 1.6614 time 810.78\n","Batch 940 Loss 1.8097 time 819.51\n","Batch 950 Loss 1.7604 time 827.80\n","Batch 960 Loss 2.0105 time 836.52\n","Batch 970 Loss 2.4154 time 845.35\n","Batch 980 Loss 1.9555 time 854.06\n","Batch 990 Loss 2.1014 time 862.43\n","Batch 1000 Loss 1.7649 time 871.16\n","Batch 1010 Loss 2.1411 time 880.08\n","Batch 1020 Loss 2.4771 time 888.33\n","Batch 1030 Loss 2.0272 time 897.00\n","Batch 1040 Loss 2.1069 time 905.57\n","Batch 1050 Loss 2.2145 time 914.48\n","Batch 1060 Loss 2.1914 time 922.75\n","Batch 1070 Loss 2.1581 time 931.73\n","Batch 1080 Loss 1.5286 time 940.47\n","Batch 1090 Loss 2.1272 time 948.83\n","Batch 1100 Loss 2.4618 time 957.64\n","Batch 1110 Loss 1.9107 time 966.53\n","Batch 1120 Loss 2.1553 time 975.35\n","Batch 1130 Loss 1.8131 time 983.56\n","Batch 1140 Loss 1.6700 time 992.38\n","Batch 1150 Loss 1.9385 time 1001.15\n","Batch 1160 Loss 2.4158 time 1009.49\n","Batch 1170 Loss 1.9512 time 1018.24\n","Batch 1180 Loss 2.0278 time 1027.23\n","Batch 1190 Loss 2.4856 time 1035.93\n","Batch 1200 Loss 1.4566 time 1044.27\n","Batch 1210 Loss 2.3041 time 1052.90\n","Batch 1220 Loss 2.6873 time 1061.36\n","Batch 1230 Loss 2.3784 time 1069.65\n","Batch 1240 Loss 2.1155 time 1078.47\n","Batch 1250 Loss 1.7787 time 1087.23\n","Batch 1260 Loss 2.0575 time 1095.84\n","Batch 1270 Loss 2.4303 time 1104.27\n","Batch 1280 Loss 2.1514 time 1112.89\n","Batch 1290 Loss 2.1616 time 1121.58\n","Batch 1300 Loss 1.9526 time 1129.79\n","Batch 1310 Loss 2.1648 time 1138.66\n","Batch 1320 Loss 2.1505 time 1147.34\n","Batch 1330 Loss 1.8206 time 1155.97\n","Batch 1340 Loss 1.7914 time 1164.34\n","Batch 1350 Loss 2.9124 time 1172.90\n","Batch 1360 Loss 2.1400 time 1181.92\n","Batch 1370 Loss 2.2323 time 1190.31\n","Batch 1380 Loss 2.2836 time 1199.21\n","Batch 1390 Loss 2.4918 time 1207.89\n","Batch 1400 Loss 2.0791 time 1216.60\n","Batch 1410 Loss 1.9938 time 1224.97\n","Batch 1420 Loss 2.1143 time 1233.89\n","Batch 1430 Loss 2.0754 time 1242.73\n","Batch 1440 Loss 1.7693 time 1251.07\n","Batch 1450 Loss 2.6011 time 1259.88\n","Batch 1460 Loss 2.3999 time 1268.57\n","Batch 1470 Loss 1.8471 time 1277.29\n","Batch 1480 Loss 2.1324 time 1285.45\n","Batch 1490 Loss 1.4513 time 1294.04\n","Batch 1500 Loss 2.1189 time 1302.77\n","Batch 1510 Loss 1.7942 time 1311.16\n","Batch 1520 Loss 1.4031 time 1319.88\n","Batch 1530 Loss 1.9334 time 1328.68\n","Batch 1540 Loss 2.7436 time 1337.38\n","Batch 1550 Loss 2.0314 time 1345.78\n","Batch 1560 Loss 1.7893 time 1354.31\n","Batch 1570 Loss 1.7556 time 1363.06\n","Batch 1580 Loss 2.3163 time 1371.29\n","Batch 1590 Loss 1.9644 time 1380.11\n","Batch 1600 Loss 1.8993 time 1388.82\n","Batch 1610 Loss 1.8209 time 1397.49\n","Batch 1620 Loss 2.4775 time 1405.85\n","Batch 1630 Loss 1.5355 time 1414.56\n","Batch 1640 Loss 2.1488 time 1423.31\n","Batch 1650 Loss 2.1111 time 1431.57\n","Batch 1660 Loss 2.4518 time 1440.33\n","Batch 1670 Loss 2.5123 time 1449.06\n","Batch 1680 Loss 2.4290 time 1457.93\n","Batch 1690 Loss 2.1814 time 1466.18\n","Batch 1700 Loss 1.5626 time 1474.89\n","Batch 1710 Loss 2.8248 time 1483.68\n","Batch 1720 Loss 1.7183 time 1492.18\n","Batch 1730 Loss 2.0375 time 1500.93\n","Batch 1740 Loss 1.7680 time 1509.89\n","Batch 1750 Loss 2.0645 time 1518.72\n","Batch 1760 Loss 2.1386 time 1527.01\n","Batch 1770 Loss 2.2915 time 1535.76\n","Batch 1780 Loss 2.7998 time 1544.30\n","Batch 1790 Loss 1.8083 time 1553.02\n","Batch 1800 Loss 1.8426 time 1561.86\n","Batch 1810 Loss 1.9399 time 1570.67\n","Batch 1820 Loss 1.6637 time 1579.36\n","Batch 1830 Loss 1.7469 time 1587.74\n","Batch 1840 Loss 1.8050 time 1596.45\n","Batch 1850 Loss 1.7568 time 1605.19\n","Batch 1860 Loss 2.2599 time 1613.40\n","Batch 1870 Loss 2.5299 time 1622.01\n","Batch 1880 Loss 2.0074 time 1630.69\n","Batch 1890 Loss 2.3224 time 1639.34\n","Batch 1900 Loss 1.7186 time 1647.75\n","Batch 1910 Loss 1.8179 time 1656.54\n","Batch 1920 Loss 2.4789 time 1665.19\n","Batch 1930 Loss 1.8479 time 1673.58\n","Batch 1940 Loss 2.2610 time 1682.33\n","Batch 1950 Loss 2.2752 time 1691.05\n","Batch 1960 Loss 1.7331 time 1699.81\n","Batch 1970 Loss 1.5006 time 1708.02\n","Batch 1980 Loss 2.7423 time 1716.81\n","Batch 1990 Loss 1.7754 time 1725.43\n","Batch 2000 Loss 1.8613 time 1733.92\n","Batch 2010 Loss 1.7405 time 1742.61\n","Batch 2020 Loss 1.5214 time 1751.23\n","Batch 2030 Loss 1.5386 time 1759.93\n","Batch 2040 Loss 1.7493 time 1768.04\n","Batch 2050 Loss 2.6832 time 1776.69\n","Batch 2060 Loss 1.5536 time 1785.35\n","Batch 2070 Loss 2.1055 time 1793.82\n","Batch 2080 Loss 1.9826 time 1802.46\n","Batch 2090 Loss 2.2854 time 1811.16\n","Batch 2100 Loss 2.2479 time 1819.90\n","Batch 2110 Loss 2.1973 time 1828.29\n","Batch 2120 Loss 2.6464 time 1837.08\n","Batch 2130 Loss 1.7531 time 1845.99\n","Batch 2140 Loss 2.0431 time 1854.26\n","Batch 2150 Loss 1.8584 time 1863.33\n","Batch 2160 Loss 1.4655 time 1872.07\n","Batch 2170 Loss 2.8959 time 1880.80\n","Batch 2180 Loss 1.9923 time 1889.03\n","Batch 2190 Loss 2.2529 time 1897.75\n","Batch 2200 Loss 1.8192 time 1906.58\n","Batch 2210 Loss 2.2945 time 1914.83\n","Batch 2220 Loss 2.1044 time 1923.58\n","Batch 2230 Loss 2.1941 time 1932.36\n","Batch 2240 Loss 2.4941 time 1941.12\n","Batch 2250 Loss 2.3301 time 1949.49\n","Batch 2260 Loss 2.1068 time 1958.42\n","Batch 2270 Loss 2.0052 time 1967.07\n","Batch 2280 Loss 2.5777 time 1975.38\n","Batch 2290 Loss 2.0601 time 1984.09\n","Batch 2300 Loss 2.1369 time 1992.95\n","Batch 2310 Loss 2.3485 time 2001.81\n","Batch 2320 Loss 1.9489 time 2010.06\n","Batch 2330 Loss 2.3603 time 2018.95\n","Batch 2340 Loss 2.4374 time 2027.75\n","Batch 2350 Loss 2.2152 time 2036.39\n","Batch 2360 Loss 2.5297 time 2045.28\n","Batch 2370 Loss 1.6587 time 2054.05\n","Batch 2380 Loss 2.4830 time 2062.77\n","Batch 2390 Loss 1.4199 time 2071.30\n","Batch 2400 Loss 1.8568 time 2080.03\n","Batch 2410 Loss 1.5663 time 2089.12\n","Batch 2420 Loss 1.8246 time 2097.48\n","Batch 2430 Loss 1.8932 time 2106.44\n","Batch 2440 Loss 1.5485 time 2115.31\n","Batch 2450 Loss 1.9835 time 2124.25\n","Batch 2460 Loss 2.5877 time 2132.52\n","Batch 2470 Loss 2.0384 time 2141.37\n","Batch 2480 Loss 2.0627 time 2150.20\n","Batch 2490 Loss 1.8867 time 2158.62\n","Batch 2500 Loss 1.9293 time 2167.56\n","Batch 2510 Loss 1.9745 time 2176.62\n","Batch 2520 Loss 2.9487 time 2185.40\n","Batch 2530 Loss 1.4959 time 2193.70\n","Batch 2540 Loss 2.0965 time 2202.50\n","Batch 2550 Loss 1.5653 time 2211.29\n","Batch 2560 Loss 1.5873 time 2219.62\n","Batch 2570 Loss 2.0245 time 2228.42\n","Batch 2580 Loss 2.0988 time 2237.23\n","Batch 2590 Loss 2.6554 time 2245.50\n","Batch 2600 Loss 1.8749 time 2254.29\n","Batch 2610 Loss 2.2197 time 2263.03\n","Batch 2620 Loss 2.0547 time 2271.31\n","Batch 2630 Loss 2.0563 time 2280.11\n","Batch 2640 Loss 2.6536 time 2289.07\n","Batch 2650 Loss 2.1112 time 2297.55\n","Batch 2660 Loss 2.3273 time 2306.44\n","Batch 2670 Loss 1.5838 time 2315.41\n","Batch 2680 Loss 1.8967 time 2323.74\n","Batch 2690 Loss 1.6929 time 2332.63\n","Batch 2700 Loss 1.8696 time 2341.39\n","Batch 2710 Loss 2.2509 time 2349.75\n","Batch 2720 Loss 1.9328 time 2358.74\n","Batch 2730 Loss 2.3237 time 2367.58\n","Batch 2740 Loss 1.8155 time 2375.87\n","Batch 2750 Loss 1.6715 time 2384.68\n","Batch 2760 Loss 1.9928 time 2393.51\n","Batch 2770 Loss 2.2082 time 2401.90\n","Batch 2780 Loss 1.3124 time 2410.82\n","Batch 2790 Loss 1.5990 time 2419.70\n","Batch 2800 Loss 1.9199 time 2428.16\n","Batch 2810 Loss 2.0675 time 2436.88\n","Batch 2820 Loss 1.7667 time 2445.63\n","Batch 2830 Loss 2.1582 time 2453.95\n","Batch 2840 Loss 2.3842 time 2462.85\n","Batch 2850 Loss 1.8814 time 2471.75\n","Batch 2860 Loss 2.0613 time 2480.37\n","Batch 2870 Loss 2.3248 time 2489.21\n","Batch 2880 Loss 2.4444 time 2497.85\n","Batch 2890 Loss 2.4664 time 2506.23\n","Batch 2900 Loss 1.7580 time 2515.02\n","Batch 2910 Loss 1.5773 time 2523.80\n","Batch 2920 Loss 1.9489 time 2531.91\n","Batch 2930 Loss 2.0777 time 2540.86\n","Batch 2940 Loss 2.2929 time 2549.69\n","Batch 2950 Loss 1.8186 time 2557.84\n","Batch 2960 Loss 2.0068 time 2566.57\n","Batch 2970 Loss 2.0173 time 2575.43\n","Batch 2980 Loss 1.6455 time 2583.63\n","Batch 2990 Loss 2.4378 time 2592.31\n","Batch 3000 Loss 2.2242 time 2601.05\n","Batch 3010 Loss 2.3120 time 2609.34\n","Batch 3020 Loss 2.1106 time 2618.08\n","Batch 3030 Loss 1.6828 time 2626.66\n","Batch 3040 Loss 2.9976 time 2635.11\n","Batch 3050 Loss 1.9197 time 2643.69\n","Batch 3060 Loss 1.4183 time 2652.41\n","Batch 3070 Loss 1.5875 time 2660.52\n","Batch 3080 Loss 1.8262 time 2669.30\n","Batch 3090 Loss 2.4593 time 2677.76\n","Batch 3100 Loss 2.0823 time 2686.04\n","Batch 3110 Loss 2.1681 time 2694.64\n","Batch 3120 Loss 2.5058 time 2703.18\n","Batch 3130 Loss 1.9025 time 2711.44\n","Batch 3140 Loss 1.9018 time 2719.93\n","Batch 3150 Loss 2.5424 time 2728.62\n","Batch 3160 Loss 1.6182 time 2736.75\n","Batch 3170 Loss 1.7003 time 2745.41\n","Batch 3180 Loss 1.6207 time 2754.06\n","Batch 3190 Loss 1.7992 time 2762.48\n","Batch 3200 Loss 1.7376 time 2771.02\n","Batch 3210 Loss 1.7933 time 2779.69\n","Batch 3220 Loss 1.6141 time 2787.90\n","Batch 3230 Loss 1.7323 time 2796.78\n","Batch 3240 Loss 1.9630 time 2805.36\n","Batch 3250 Loss 1.6385 time 2813.47\n","Batch 3260 Loss 1.8227 time 2822.08\n","Batch 3270 Loss 1.6705 time 2830.67\n","Batch 3280 Loss 2.0617 time 2838.94\n","Batch 3290 Loss 1.5610 time 2847.76\n","Batch 3300 Loss 2.3029 time 2856.36\n","Batch 3310 Loss 2.0653 time 2864.54\n","Batch 3320 Loss 1.7568 time 2873.28\n","Batch 3330 Loss 2.0584 time 2881.95\n","Batch 3340 Loss 1.7718 time 2890.37\n","Batch 3350 Loss 2.5202 time 2899.10\n","Batch 3360 Loss 1.8865 time 2907.77\n","Batch 3370 Loss 2.3415 time 2915.94\n","Batch 3380 Loss 1.7815 time 2924.73\n","Batch 3390 Loss 2.1991 time 2933.37\n","Batch 3400 Loss 1.9541 time 2941.52\n","Batch 3410 Loss 2.2579 time 2950.47\n","Batch 3420 Loss 2.0926 time 2959.09\n","Batch 3430 Loss 1.9261 time 2967.35\n","Batch 3440 Loss 1.8403 time 2975.99\n","Batch 3450 Loss 2.1295 time 2984.64\n","Batch 3460 Loss 3.2922 time 2993.08\n","Batch 3470 Loss 1.7037 time 3001.58\n","Batch 3480 Loss 1.6930 time 3010.35\n","Batch 3490 Loss 1.8851 time 3018.70\n","Batch 3500 Loss 2.6122 time 3027.36\n","Batch 3510 Loss 1.5788 time 3035.83\n","Batch 3520 Loss 1.9750 time 3044.17\n","Batch 3530 Loss 1.5990 time 3052.90\n","Batch 3540 Loss 2.6937 time 3061.59\n","Batch 3550 Loss 1.3767 time 3069.88\n","Batch 3560 Loss 2.4241 time 3078.63\n","Batch 3570 Loss 1.7380 time 3087.30\n","Batch 3580 Loss 1.8987 time 3095.39\n","Batch 3590 Loss 2.1030 time 3104.38\n","Batch 3600 Loss 2.5590 time 3113.24\n","Batch 3610 Loss 2.5515 time 3121.55\n","Batch 3620 Loss 2.1934 time 3130.29\n","Batch 3630 Loss 2.1646 time 3139.02\n","Batch 3640 Loss 2.1414 time 3147.18\n","Batch 3650 Loss 1.4826 time 3155.94\n","Batch 3660 Loss 2.7698 time 3164.63\n","Batch 3670 Loss 2.0457 time 3173.00\n","Batch 3680 Loss 2.4381 time 3181.71\n","Batch 3690 Loss 2.3025 time 3190.39\n","Batch 3700 Loss 2.4250 time 3198.76\n","Batch 3710 Loss 2.0435 time 3207.51\n","Batch 3720 Loss 1.9374 time 3216.28\n","Batch 3730 Loss 2.0208 time 3224.66\n","Batch 3740 Loss 2.7772 time 3233.45\n","Batch 3750 Loss 2.6798 time 3242.14\n","Batch 3760 Loss 2.2455 time 3250.58\n","Batch 3770 Loss 2.1417 time 3259.31\n","Batch 3780 Loss 1.8848 time 3268.18\n","Batch 3790 Loss 2.0118 time 3276.39\n","Batch 3800 Loss 2.3066 time 3285.10\n","Batch 3810 Loss 2.7102 time 3294.00\n","Batch 3820 Loss 1.7955 time 3302.24\n","Batch 3830 Loss 2.1750 time 3311.04\n","Batch 3840 Loss 1.6106 time 3319.91\n","Batch 3850 Loss 1.8943 time 3328.28\n","Batch 3860 Loss 1.9974 time 3336.92\n","Batch 3870 Loss 2.6034 time 3345.77\n","Batch 3880 Loss 2.4198 time 3354.15\n","Batch 3890 Loss 1.9767 time 3362.93\n","Batch 3900 Loss 1.7992 time 3371.64\n","Batch 3910 Loss 1.8602 time 3379.85\n","Batch 3920 Loss 2.1299 time 3388.73\n","Batch 3930 Loss 2.3232 time 3397.51\n","Batch 3940 Loss 2.0349 time 3406.00\n","Batch 3950 Loss 2.3637 time 3415.07\n","Batch 3960 Loss 2.8192 time 3423.99\n","Batch 3970 Loss 2.0967 time 3432.37\n","Batch 3980 Loss 1.7397 time 3441.14\n","Batch 3990 Loss 2.4723 time 3450.06\n","Batch 4000 Loss 1.6505 time 3458.49\n","Batch 4010 Loss 2.0137 time 3467.36\n","Batch 4020 Loss 2.6714 time 3476.25\n","Batch 4030 Loss 2.1282 time 3484.68\n","Batch 4040 Loss 1.5058 time 3493.46\n","Batch 4050 Loss 2.2006 time 3502.21\n","Batch 4060 Loss 2.0380 time 3510.78\n","Batch 4070 Loss 2.5867 time 3519.61\n","Batch 4080 Loss 2.3073 time 3528.36\n","Batch 4090 Loss 2.1321 time 3536.78\n","Batch 4100 Loss 3.0532 time 3545.49\n","Batch 4110 Loss 2.7053 time 3554.26\n","Batch 4120 Loss 1.9754 time 3562.46\n","Batch 4130 Loss 2.7505 time 3571.22\n","Batch 4140 Loss 2.3360 time 3579.97\n","Batch 4150 Loss 2.1092 time 3588.45\n","Batch 4160 Loss 2.5360 time 3597.22\n","Batch 4170 Loss 2.1863 time 3606.01\n","Batch 4180 Loss 1.9516 time 3614.30\n","Batch 4190 Loss 2.0532 time 3623.20\n","Batch 4200 Loss 1.7715 time 3631.89\n","Batch 4210 Loss 1.8797 time 3640.31\n","Batch 4220 Loss 1.9675 time 3648.98\n","Batch 4230 Loss 2.2248 time 3657.64\n","Batch 4240 Loss 3.0703 time 3665.95\n","Batch 4250 Loss 1.8697 time 3674.57\n","Batch 4260 Loss 1.5633 time 3683.20\n","Batch 4270 Loss 2.0105 time 3691.27\n","Batch 4280 Loss 2.2903 time 3699.78\n","Batch 4290 Loss 2.2927 time 3708.40\n","Batch 4300 Loss 2.0054 time 3716.84\n","Batch 4310 Loss 1.8474 time 3725.71\n","Batch 4320 Loss 2.0347 time 3734.29\n","Batch 4330 Loss 1.9947 time 3742.50\n","Batch 4340 Loss 2.0782 time 3751.22\n","Batch 4350 Loss 1.9883 time 3759.82\n","Batch 4360 Loss 1.8087 time 3768.03\n","Batch 4370 Loss 2.2497 time 3776.74\n","Batch 4380 Loss 2.2018 time 3785.22\n","Batch 4390 Loss 2.3568 time 3793.50\n","Batch 4400 Loss 1.9623 time 3802.07\n","Batch 4410 Loss 2.0267 time 3810.67\n","Batch 4420 Loss 2.0622 time 3818.71\n","Batch 4430 Loss 2.2469 time 3827.44\n","Batch 4440 Loss 2.4518 time 3836.11\n","Batch 4450 Loss 2.2356 time 3844.34\n","Batch 4460 Loss 1.7913 time 3852.88\n","Batch 4470 Loss 1.9638 time 3861.62\n","Batch 4480 Loss 2.0620 time 3869.80\n","Batch 4490 Loss 2.6488 time 3878.31\n","Batch 4500 Loss 1.6404 time 3886.84\n","Batch 4510 Loss 2.3641 time 3895.00\n","Batch 4520 Loss 2.1466 time 3903.63\n","Batch 4530 Loss 1.7612 time 3912.21\n","Batch 4540 Loss 1.9492 time 3920.53\n","Batch 4550 Loss 1.8610 time 3929.12\n","Batch 4560 Loss 2.2671 time 3937.72\n","Batch 4570 Loss 1.7743 time 3945.93\n","Batch 4580 Loss 1.7178 time 3954.52\n","Batch 4590 Loss 1.9409 time 3963.16\n","Batch 4600 Loss 2.0590 time 3971.39\n","Batch 4610 Loss 2.0740 time 3980.07\n","Batch 4620 Loss 2.2868 time 3988.65\n","Batch 4630 Loss 1.9430 time 3996.94\n","Batch 4640 Loss 1.9050 time 4005.63\n","Batch 4650 Loss 2.7810 time 4014.37\n","Batch 4660 Loss 2.0861 time 4022.52\n","Batch 4670 Loss 1.7676 time 4031.26\n","Batch 4680 Loss 3.0004 time 4040.10\n","Batch 4690 Loss 2.2024 time 4048.14\n","Batch 4700 Loss 1.9289 time 4056.92\n","Batch 4710 Loss 1.8227 time 4065.67\n","Batch 4720 Loss 1.9355 time 4074.02\n","Batch 4730 Loss 1.5460 time 4082.47\n","Batch 4740 Loss 1.8309 time 4091.07\n","Batch 4750 Loss 2.0190 time 4099.23\n","Batch 4760 Loss 2.0021 time 4107.96\n","Batch 4770 Loss 1.6812 time 4116.55\n","Batch 4780 Loss 1.9124 time 4124.83\n","Batch 4790 Loss 2.2976 time 4133.46\n","Batch 4800 Loss 1.9726 time 4141.98\n","Batch 4810 Loss 2.0703 time 4150.35\n","Batch 4820 Loss 2.1355 time 4158.87\n","Batch 4830 Loss 1.9710 time 4167.49\n","Batch 4840 Loss 2.0133 time 4175.58\n","Batch 4850 Loss 2.1844 time 4184.36\n","Batch 4860 Loss 2.2750 time 4192.91\n","Batch 4870 Loss 2.3296 time 4201.12\n","Batch 4880 Loss 2.0382 time 4209.90\n","Batch 4890 Loss 1.9056 time 4218.53\n","Batch 4900 Loss 1.8812 time 4226.66\n","Batch 4910 Loss 1.8979 time 4235.45\n","Batch 4920 Loss 2.2654 time 4244.08\n","Batch 4930 Loss 1.6846 time 4252.28\n","Batch 4940 Loss 3.0682 time 4260.97\n","Batch 4950 Loss 2.3536 time 4269.60\n","Batch 4960 Loss 2.1843 time 4277.87\n","Batch 4970 Loss 2.1214 time 4286.42\n","Batch 4980 Loss 2.3400 time 4294.95\n","Batch 4990 Loss 2.0422 time 4303.05\n","Batch 5000 Loss 2.4654 time 4311.67\n","Batch 5010 Loss 2.8306 time 4320.40\n","Batch 5020 Loss 2.1645 time 4328.60\n","Batch 5030 Loss 2.0981 time 4337.21\n","Batch 5040 Loss 1.7238 time 4346.04\n","Batch 5050 Loss 2.2919 time 4354.26\n","Batch 5060 Loss 1.8340 time 4362.74\n","Batch 5070 Loss 1.6258 time 4371.44\n","Batch 5080 Loss 2.0493 time 4379.54\n","Batch 5090 Loss 2.0335 time 4388.16\n","Batch 5100 Loss 2.2627 time 4396.83\n","Batch 5110 Loss 2.0316 time 4404.90\n","Batch 5120 Loss 2.3480 time 4413.42\n","Batch 5130 Loss 2.7550 time 4422.05\n","Batch 5140 Loss 1.8499 time 4430.45\n","Batch 5150 Loss 1.8751 time 4439.14\n","Batch 5160 Loss 2.0934 time 4447.86\n","Batch 5170 Loss 1.9031 time 4456.01\n","Loss at epoch end 2.1241\n","Model: \"unidir_gru_encoder\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        multiple                  6358500   \n","_________________________________________________________________\n","gru (GRU)                    multiple                  428544    \n","=================================================================\n","Total params: 6,787,044\n","Trainable params: 6,787,044\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"uni_gru_decoder_with_attention\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      multiple                  6358500   \n","_________________________________________________________________\n","gru_1 (GRU)                  multiple                  625152    \n","_________________________________________________________________\n","dense (Dense)                multiple                  5447115   \n","_________________________________________________________________\n","bahdanau_attention (Bahdanau multiple                  131841    \n","=================================================================\n","Total params: 12,562,608\n","Trainable params: 12,562,608\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rYeJ5aTTUxmN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y7OfsVX-mQOw","colab_type":"code","outputId":"a6976b62-8e11-4013-d30c-effe2112197a","executionInfo":{"status":"ok","timestamp":1576830911182,"user_tz":-480,"elapsed":6324,"user":{"displayName":"Luo Yongliang","photoUrl":"","userId":"08228183385143629193"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["encoder, decoder = create_codec()\n","train_for_restore(encoder, decoder, get_dataset())"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=8.390885>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"7jW9s_c4auyZ","colab_type":"code","colab":{}},"source":["encoder.trainable_variables"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWA6Uo2D2usE","colab_type":"code","colab":{}},"source":["decoder.trainable_variables"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ck5kyNS5nGUs","colab_type":"code","colab":{}},"source":["print(checkpoint_prefix)\n","print(checkpoint_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XOUeVod0zylj","colab_type":"code","outputId":"1843ef1d-4e55-4ce8-875b-97526e3abbd4","executionInfo":{"status":"ok","timestamp":1576830947927,"user_tz":-480,"elapsed":2841,"user":{"displayName":"Luo Yongliang","photoUrl":"","userId":"08228183385143629193"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["checkpoint = def_checkpoint(encoder, decoder, optimizer)\n","\n","load_status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n","load_status.assert_consumed()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f12ab0b66d8>"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"GYJbc8g3ZVGs","colab_type":"code","colab":{}},"source":["decoder.embedding.embeddings"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cAZYWDeXg4GT","colab_type":"code","colab":{}},"source":["import os\n","from tensorflow.python import pywrap_tensorflow\n","# checkpoint_path = os.path.join(model_dir, \"model.ckpt\")\n","checkpoint_path = checkpoint_prefix\n","reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path)\n","var_to_shape_map = reader.get_variable_to_shape_map()\n","for key in var_to_shape_map:\n","    print(\"\\r\\ntensor_name: \", key, end=' ')\n","    print(reader.get_tensor(key))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1414MQTFmnm","colab_type":"code","colab":{}},"source":["load_status.assert_existing_objects_matched()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SKqKdFuJPdyb","colab_type":"code","colab":{}},"source":["doc = \"更好机油散热器，问题依然没有解决技师说仪表亮红色机油灯？|车主说|车主说滴滴几声恢复正常。请参考原先提问！|技师说到哪去找。|车主说图片|车主说看到？|技师说领驭？|车主说|车主说感觉行驶一段时间后报警。走走停停报警次数增多。|技师说稍|技师说图片|技师说图片|技师说测量一下标准机油压力，压力低需要进行相应检查维修。|车主说凉车机油压力4|车主说热车才1|技师说热车怠速最低1.3|车主说不了1.3|技师说发动机水温？|车主说水温正常|车主说没有测量温度|技师说水温80度机油压力不到1.3，说明卸压，水温90以上，不到1.3说明散热不好。|车主说好|车主说再试试|车主说两种情况处理？|技师说冷车机油粘度大，压力肯定高|车主说两种情况解决？|技师说水温80度，机油压力灯亮，说明机械方面故障，考虑机油泵，限压阀，机油喷嘴，热车机油压力灯亮，重点考虑机油粘度机油散热器，水箱是否堵塞，机油里加注抗磨剂。\"\n","ts = document_as_tensor(doc)\n","evaluate(encoder, decoder, doc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b2uj42HKQdEa","colab_type":"code","colab":{}},"source":["ts"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WuCO7RbmiwKa","colab_type":"code","colab":{}},"source":["decoder = UniGruDecoderWithAttention(vocab_tar_size, embedding_dim, working_ds.embedding_matrix, codec_units, BATCH_SIZE)\n","\n","sample_decoder_output, state, attention_weights = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n","                                      sample_hidden, sample_output)\n","decoder.summary()\n","print ('UniGruDecoderWithAttention output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n","print ('UniGruDecoderWithAttention state shape: (batch_size, vocab size) {}'.format(state.shape))\n","print ('UniGruDecoderWithAttention attention_weights shape: (batch_size, vocab size) {}'.format(attention_weights.shape))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rc4TTq1CiwKd","colab_type":"code","colab":{}},"source":["attention_layer = BahdanauAttention(10)\n","attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n","attention_layer.summary()\n","\n","print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n","print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sy6bFkRm6HLD","colab_type":"code","colab":{}},"source":["21195*300"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jD2lhiUqiwKf","colab_type":"code","colab":{}},"source":["dataset = tf.data.Dataset.from_tensor_slices((working_ds.train_ids_x, working_ds.train_ids_y)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","example_input_batch, example_target_batch = next(iter(dataset))\n","print(example_input_batch.shape, example_target_batch.shape)\n","print(example_input_batch)\n","\n","encoder = UnidirGruEncoder(vocab_inp_size, embedding_dim, working_ds.embedding_matrix, codec_units, BATCH_SIZE)\n","\n","# sample input\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n","\n","encoder.summary()\n","print ('ThisEncoder input shape: (batch size, sequence length) {}'.format(example_input_batch.shape))\n","print ('ThisEncoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n","print ('ThisEncoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n","print(type(encoder.embedding.embeddings))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zfRvfoSPiwKi","colab_type":"code","colab":{}},"source":["# !pip show tensorflow-gpu\n","# tf.__version__\n","# gpus = tf.config.experimental.list_physical_devices('GPU')\n","# print(gpus)\n","# tf.test.is_gpu_available( cuda_only=False, min_cuda_compute_capability=None )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iHF0pFPsiwKk","colab_type":"code","colab":{}},"source":["# -*- coding:utf-8 -*-\n","# Created by Kevin Luo at 2019-12-17\n","import tensorflow as tf\n","\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import sparse_categorical_crossentropy\n","\n","\n","def seq2seq(input_length, output_sequence_length, embedding_matrix, vocab_size):\n","    model = Sequential()\n","    model.add(Embedding(input_dim=vocab_size, output_dim=300, weights=[embedding_matrix], trainable=False,\n","                        input_length=input_length))\n","    model.add(Bidirectional(GRU(300, return_sequences=False)))\n","    model.add(Dense(300, activation=\"relu\"))\n","    model.add(RepeatVector(output_sequence_length))\n","    model.add(Bidirectional(GRU(300, return_sequences=True)))\n","    model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n","    model.compile(loss=sparse_categorical_crossentropy,\n","                  optimizer=Adam(1e-3))\n","    model.summary()\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VgxgupwLiwKp","colab_type":"code","colab":{}},"source":["print(working_ds.train_df['X'].iloc[4999])\n","\n","working_ds.wv_model.most_similar(positive=['语音'], negative=[])\n","\n","print(working_ds.train_csv)\n","print(working_ds.test_csv)\n","        \n","print(working_ds.train_df.shape)\n","print(working_ds.test_df.shape)\n","print(working_ds.merged_df.shape)\n","        \n","print(working_ds.train_y_max_len)\n","        \n","print(working_ds.X_max_len)\n","\t\t\n","print(working_ds.train_X.shape)\n","print(working_ds.train_Y.shape)\n","print(working_ds.test_X.shape)\n","\n","print(working_ds.train_ids_x.shape)\n","print(working_ds.train_ids_y.shape)\n","print(working_ds.test_ids_x.shape)\n","\t\t\n","print(working_ds.wv_model)\n","\t\t\n","print(len(working_ds.vocab))\n","print(len(working_ds.reverse_vocab))\n","\t\t\n","print(working_ds.embedding_matrix.shape)\n","\n","print(working_ds.train_ids_x.shape)\n","print(working_ds.train_ids_y.shape)\n","print(working_ds.test_ids_x.shape)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6AlXOtZViwKs","colab_type":"code","colab":{}},"source":["basic_s2s_model = seq2seq(working_ds.X_max_len, working_ds.train_y_max_len, working_ds.embedding_matrix, len(working_ds.vocab))\n","\n","basic_s2s_model.fit(working_ds.train_X, working_ds.train_Y, batch_size=32, epochs=1, validation_split=0.2)\n","basic_s2s_model.save(os.path.join(project_root, r'data\\seq2seq_basic_s2s.model'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DqnAN2vCiwKw","colab_type":"code","colab":{}},"source":["working_ds.wv_model.most_similar(positive=[''], negative=[])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0JaMUyPGiwKz","colab_type":"code","colab":{}},"source":["print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"],"execution_count":0,"outputs":[]}]}